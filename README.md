# AI-Engineer-Note

Tất cả những thứ sưu tầm được liên quan đến AI Engineer và Deploy Services 
- [Linux & CUDA & APT-Packages](Linux)
    + [FAQ](Linux/)
- [Deeplearning](Deeplearning)
    + [1. ComputerVision](Deeplearning/ComputerVision)
        + [1.1 Common Architectures](Deeplearning/ComputerVision)
            + [1.1.1 ResBlock](Deeplearning/ComputerVision/docs/resblock.md)
            + [1.1.2 Gated Convolution](Deeplearning/ComputerVision/docs/gated_convolution.md)
            + [1.1.3 Multi-head Attention](Deeplearning/ComputerVision/docs/multihead_attn.md)
    + [2. NLP](Deeplearning/NLP)
- [Frameworks](Framework)
    + [1. TensorRT](Framework/TensorRT)
        + [1.1 Convert ONNX model to TensorRT](Framework/TensorRT/docs/tutorial.md)
        + [1.2 Wrapped TensorRT-CPP Models](https://github.com/NNDam/TensorRT-CPP)
            + [1.2.1 Arcface](https://github.com/NNDam/TensorRT-CPP/tree/main/Arcface)
            + [1.2.2 SCRFD](https://github.com/NNDam/TensorRT-CPP/tree/main/SCRFD)
            + [1.2.3 YOLOv7](https://github.com/NNDam/TensorRT-CPP/tree/main/YOLOv7)
    + [2. Pytorch](Framework/Pytorch)
        + [2.1 Build Pytorch from source (Optimize speed for AMD CPU & NVIDIA GPU)](Framework/Pytorch/docs/build_from_source.md)
- [Deploy](Deploy)
    + [1. NVIDIA](Deploy/NVIDIA)
        + [1.1 Multi-instance GPU (MIG)](Deploy/NVIDIA/docs/multi_instance_gpu.md)
        + [1.2 FFMPEG with Nvidia hardware-acceleration](Deploy/NVIDIA/docs/nvidia_video_sdk.md)
    + [2. Deepstream](Deploy/Deepstream)
        + [2.1 Yolov4](Deploy/Deepstream/sample-yolov4)
        + [2.2 Traffic Analyst](Deploy/Deepstream/sample-ALPR)
        + [2.3 SCRFD Face Detection (custom parser & NMS plugin with landmark)](Deploy/Deepstream/sample-scrfd)
    + [3. Triton Inference Server](Deploy/Triton-inference-server)
        - [3.1 Cài đặt triton-server và triton-client](Deploy/Triton-inference-server/docs/install.md)
            + [3.1.1 Các chế độ quản lý model (load/unload/reload)](Deploy/Triton-inference-server/docs/model_management.md)
        - [3.2 Sơ lược về các backend trong Triton](Deploy/Triton-inference-server/docs/backend.md)
        - [3.3 Cấu hình cơ bản khi deploy mô hình](Deploy/Triton-inference-server/docs/model_configuration.md)
        - [3.4 Deploy mô hình](#)
            - [3.4.1 ONNX-runtime](Deploy/Triton-inference-server/docs/triton_onnx.md)
            - [3.4.2 TensorRT](Deploy/Triton-inference-server/docs/triton_tensorrt.md)
            - [3.4.3 Pytorch & TorchScript](Deploy/Triton-inference-server/docs/triton_pytorch.md)
            - [3.4.4 Kaldi <i>(Advanced)</i>](Deploy/Triton-inference-server/docs/triton_kaldi.md)
        - [3.5 Model Batching](Deploy/Triton-inference-server/docs/model_batching.md)
        - [3.6 Ensemble Model và pre/post processing](Deploy/Triton-inference-server/docs/model_ensemble.md)
        - [3.7 Sử dụng Performance Analyzer Tool](Deploy/Triton-inference-server/docs/perf_analyzer.md)
        - [3.8 Optimizations](#)
            + [3.8.1 Tối ưu Pytorch backend](Deploy/Triton-inference-server/docs/optimization_pytorch.md)
    + [4. TAO Toolkit (Transfer-Learning-Toolkit)](Deploy/Transfer-Learning-Toolkit)
